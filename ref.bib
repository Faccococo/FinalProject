@article{Wang2024,
  author    = {Y. Wang and H. Zhang},
  title     = {Paintings in naked-eye virtual reality: A parallax view between the surface and volumetric depth},
  journal   = {Humanities and Social Sciences Communications},
  volume    = {11},
  number    = {1},
  pages     = {233},
  year      = {2024},
  doi       = {10.1057/s41599-024-02697-z},
  url       = {https://doi.org/10.1057/s41599-024-02697-z}
}@misc{ahn_opengl_projection_matrix,
  author = {Song Ho Ahn},
  title = {OpenGL Projection Matrix},
  year = {2024},
  url = {https://www.songho.ca/opengl/gl_projectionmatrix.html},
  note = {Accessed: 2024-05-16}
}
@misc{aptas_off_axis_projection_2024,
  author       = {APTAS},
  title        = {Off-Axis Projection Unity},
  year         = {2024},
  howpublished = {\url{https://github.com/aptas/off-axis-projection-unity}},
  note         = {Accessed: 2024-05-16}
}

@article{saberian2014boosting,
  title={Boosting algorithms for detector cascade learning},
  author={Saberian, Mohammad and Vasconcelos, Nuno},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={2569--2605},
  year={2014},
  publisher={JMLR. org}
}
@inproceedings{Kooima2011GeneralizedPP,
  title={Generalized Perspective Projection},
  author={Robert Kooima},
  year={2011},
  url={https://api.semanticscholar.org/CorpusID:708371}
}

@misc{soheil2023facial,
  author = {Soheil M. Pour},
  title = {Facial Keypoint Detection},
  year = {2023},
  month = {April},
  url = {https://github.com/soheil-mp/Facial-Keypoint-Detection},
  note = {GitHub repository}
}


@misc{ros2,
  author = {Open Source Robotics Foundation},
  title = {ROS 2: Robot Operating System},
  year = {2024},
  url = {https://docs.ros.org/en/ros2_documentation/index.html},
  note = {Accessed: 2024-05-15}
}
@misc{ros2forunity,
  author = {Robotec.AI},
  title = {ROS 2 for Unity},
  year = {2024},
  url = {https://github.com/RobotecAI/ros2-for-unity},
  note = {Accessed: 2024-05-15}
}
@misc{chen2022opencv,
  author = {Zhangjie Chen},
  title = {OpenCV Face Detection: Cascade Classifier vs. YuNet},
  year = {2022},
  month = {November},
  url = {https://opencv.org/blog/opencv-face-detection-cascade-classifier-vs-yunet/},
  note = {OpenCV Blog}
}
@INPROCEEDINGS{off-axis,
  author={Hancock, Mark and Carpendale, Sheelagh},
  booktitle={Second Annual IEEE International Workshop on Horizontal Interactive Human-Computer Systems (TABLETOP'07)}, 
  title={Supporting Multiple Off-Axis Viewpoints at a Tabletop Display}, 
  year={2007},
  volume={},
  number={},
  pages={171-178},
  keywords={Three dimensional displays;Collaboration;Large screen displays;Cameras;Conferences;Collaborative work;Stacking;Two dimensional displays;Rendering (computer graphics);Virtual reality},
  doi={10.1109/TABLETOP.2007.9}}

@misc{Lee,
  author = {Lee, J. C.},
  title = {Head tracking for desktop VR displays using the Wii remote},
  howpublished = {\url{http://johnnylee.net/projects/wii/}},
  note = {Available online}
}
@manual{Apple2014,
  title = {iPhone User Guide For iOS 8.1 Software},
  author = {{Apple Inc.}},
  year = {2014},
  organization = {Apple Inc.}
}

@article{Zhu2016,
  author = {Zhu, Zhe and Martin, Ralph R. and Pepperell, Robert and Burleigh, Alistair},
  title = {3D modeling and motion parallax for improved videoconferencing},
  journal = {Computational Visual Media},
  year = {2016},
  volume = {2},
  number = {2},
  pages = {131--142},
  month = {June},
  abstract = {We consider a face-to-face videoconferencing system that uses a Kinect camera at each end of the link for 3D modeling and an ordinary 2D display for output. The Kinect camera allows a 3D model of each participant to be transmitted; the (assumed static) background is sent separately. Furthermore, the Kinect tracks the receiver’s head, allowing our system to render a view of the sender depending on the receiver’s viewpoint. The resulting motion parallax gives the receivers a strong impression of 3D viewing as they move, yet the system only needs an ordinary 2D display. This is cheaper than a full 3D system, and avoids disadvantages such as the need to wear shutter glasses, VR headsets, or to sit in a particular position required by an autostereo display. Perceptual studies show that users experience a greater sensation of depth with our system compared to a typical 2D videoconferencing system.},
  issn = {2096-0662},
  doi = {10.1007/s41095-016-0038-4},
  url = {https://doi.org/10.1007/s41095-016-0038-4}
}

@article{Hanes2008,
  author = {Hanes, Douglas A. and Keller, Julia and McCollum, Gin},
  title = {Motion parallax contribution to perception of self-motion and depth},
  journal = {Biological Cybernetics},
  year = {2008},
  volume = {98},
  number = {4},
  pages = {273--293},
  month = {April},
  abstract = {The object of this study is to mathematically specify important characteristics of visual flow during translation of the eye for the perception of depth and self-motion. We address various strategies by which the central nervous system may estimate self-motion and depth from motion parallax, using equations for the visual velocity field generated by translation of the eye through space. Our results focus on information provided by the movement and deformation of three-dimensional objects and on local flow behavior around a fixated point. All of these issues are addressed mathematically in terms of definite equations for the optic flow. This formal characterization of the visual information presented to the observer is then considered in parallel with other sensory cues to self-motion in order to see how these contribute to the effective use of visual motion parallax, and how parallactic flow can, conversely, contribute to the sense of self-motion. This article will focus on a central case, for understanding of motion parallax in spacious real-world environments, of monocular visual cues observable during pure horizontal translation of the eye through a stationary environment. We suggest that the global optokinetic stimulus associated with visual motion parallax must converge in significant fashion with vestibular and proprioceptive pathways that carry signals related to self-motion. Suggestions of experiments to test some of the predictions of this study are made.},
  issn = {1432-0770},
  doi = {10.1007/s00422-008-0224-2},
  url = {https://doi.org/10.1007/s00422-008-0224-2}
}
@misc{ros2_camera_calibration,
  author       = {Open Robotics},
  title        = {ROS 2 Camera Calibration Package},
  howpublished = {\url{https://1.com/ros-perception/image_pipeline}},
  year         = {2023},
  note         = {Accessed: 2024-05-15}
}


@misc{TheParallaxView,
  author = {Pader Norrby},
  title = {TheParallaxView},
  year = {2024},
  howpublished = {\url{https://github.com/algomystic/TheParallaxView}},
  note = {Accessed: 2024-05-14}
}

@article{EPnP_2009, 
  title={EPnP: An Accurate O(n) Solution to the PnP Problem},
  author={Lepetit, Vincent and Moreno-Noguer, Francesc and Fua, Pascal},
  journal={International Journal of Computer Vision},
  volume={81},
  number={2},
  pages={155--166},
  year={2009},
  publisher={Springer},
  doi={10.1007/s11263-008-0152-6},
  url={https://doi.org/10.1007/s11263-008-0152-6}
}
@article{ALLISON20031879,
title = {Geometric and induced effects in binocular stereopsis and motion parallax},
journal = {Vision Research},
volume = {43},
number = {17},
pages = {1879-1893},
year = {2003},
issn = {0042-6989},
doi = {https://doi.org/10.1016/S0042-6989(03)00298-0},
url = {https://www.sciencedirect.com/science/article/pii/S0042698903002980},
author = {Robert S Allison and Brian J Rogers and Mark F Bradshaw},
abstract = {This paper examines and contrasts motion-parallax analogues of the induced-size and induced-shear effects with the equivalent induced effects from binocular disparity. During lateral head motion or with binocular stereopsis, vertical-shear and vertical-size transformations produced ‘induced effects’ of apparent inclination and slant that are not predicted geometrically. With vertical head motion, horizontal-shear and horizontal-size transformations produced similar analogues of the disparity induced effects. Typically, the induced effects were opposite in direction and slightly smaller in size than the geometric effects. Local induced-shear and induced-size effects could be elicited from motion parallax, but not from disparity, and were most pronounced when the stimulus contained discontinuities in velocity gradient. The implications of these results are discussed in the context of models of depth perception from disparity and structure from motion.}
}
@article{Wang_2018,
doi = {10.1088/1742-6596/1098/1/012013},
url = {https://dx.doi.org/10.1088/1742-6596/1098/1/012013},
year = {2018},
month = {sep},
publisher = {IOP Publishing},
volume = {1098},
number = {1},
pages = {012013},
author = {Ziyao Wang and Haikun Wei},
title = {Naked Eye Pseudo 3D Display Technology Outside the Screen},
journal = {Journal of Physics: Conference Series},
abstract = {The 3D display technology based on motion parallax has the advantages of no need to wear glasses and has no limitation on the viewing angle. However, the current problems of this method include 3D scenes and models can only be displayed within the screen; the system is sensitive to light. For the first problem, we proposed the concept of virtual bezel, which makes out system could produce a strong illusion that objects displayed outside the screen, and greatly increases the visual impact of 3D display. For the problem of light sensitivity, unlike other system using RGB camera, we use RGB-D images to detect and track the viewer’s face, which makes our system work properly even at night.}
}

@article{Khabarlak_2022,
  title     = {Fast Facial Landmark Detection and Applications: A Survey},
  volume    = {22},
  issn      = {1666-6046},
  url       = {http://dx.doi.org/10.24215/16666038.22.e02},
  doi       = {10.24215/16666038.22.e02},
  number    = {1},
  journal   = {Journal of Computer Science and Technology},
  publisher = {Universidad Nacional de La Plata},
  author    = {Khabarlak, Kostiantyn and Koriashkina, Larysa},
  year      = {2022},
  month     = apr,
  pages     = {e02}
}
@article{Kalman1960,  
    author = {Kalman, R. E.},
    title = "{A New Approach to Linear Filtering and Prediction Problems}",
    journal = {Journal of Basic Engineering},
    volume = {82},
    number = {1},
    pages = {35-45},
    year = {1960},
    month = {03},
    abstract = "{The classical filtering and prediction problem is re-examined using the Bode-Shannon representation of random processes and the “state-transition” method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinite-memory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the co-efficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix.}",
    issn = {0021-9223},
    doi = {10.1115/1.3662552},
    url = {https://doi.org/10.1115/1.3662552},
    eprint = {https://asmedigitalcollection.asme.org/fluidsengineering/article-pdf/82/1/35/5518977/35\_1.pdf},
}
@article{KumarMondal2021,
  author = {Manav Kumar and Sharifuddin Mondal},
  title = {Recent developments on target tracking problems: A review},
  journal = {Ocean Engineering},
  volume = {236},
  year = {2021},
  pages = {109558},
  issn = {0029-8018},
  doi = {https://doi.org/10.1016/j.oceaneng.2021.109558},
  url = {https://www.sciencedirect.com/science/article/pii/S0029801821009471},
  abstract = {Recent progresses in the target tracking technology have changed current unmanned systems into a realistic substitute to the conventional tracking systems. In this paper, existing algorithms on target tracking for both aerial and underwater application scenarios are classified based on the active and passive modes of target tracking. These algorithms are analysed and compared in the form of mode, tracking technology and respective validation algorithm available in the literature. From this survey, the future directions and major challenges are edged to obtain higher level of tracking performance.},
  keywords = {AUV; UAV; AUAV; PUAV; MTT}
}

@article{Qi2022YOLO5Face,
  title={YOLO5Face: Why Reinventing a Face Detector},
  author={Qi, Delong and Tan, Weijun and Yao, Qi and Liu, Jingfeng},
  journal={arXiv preprint arXiv:2105.12931},
  year={2022},
  institution={Shenzhen Deepcam Information Technologies, Shenzhen, China and LinkSprite Technologies, USA},
  abstract={In this paper, we extend the YOLOv5 object detection framework to develop YOLO5Face, a robust face detector with landmark regression capabilities. We introduce modifications tailored for detecting faces of various sizes and conditions, achieving state-of-the-art performance on the WiderFace dataset. Our model is designed to operate efficiently on both high-performance systems and embedded or mobile devices.},
  url={https://arxiv.org/abs/2105.12931}
}

@misc{YOLOv8Face,
  author = {Qi, Delong},
  title = {YOLOv8-Face: A Face Detection Model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/derronqi/yolov8-face}},
  commit = {last commit hash (optional)},
  note = {Accessed: 2024-05-09}
}

@misc{YOLOv7Face,
  author = {Qi, Delong},
  title = {YOLOv7-Face: A Face Detection Model},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/derronqi/yolov7-face}},
  commit = {last commit hash (optional)},
  note = {Accessed: 2024-05-09}
}

@article{Wu_2023,
  title   = {YuNet: A Tiny Millisecond-level Face Detector},
  volume  = {20},
  issn    = {1673-4785},
  url     = {https://doi.org/10.1007/s11633-023-1423-y},
  doi     = {10.1007/s11633-023-1423-y},
  number  = {5},
  journal = {Machine Intelligence Research},
  author  = {Wu, Wei and Peng, Hui and Yu, Shiqi},
  year    = {2023},
  month   = oct,
  pages   = {656-665}
}
@inproceedings{He_2016_CVPR,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title     = {Deep Residual Learning for Image Recognition},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2016}
}
@article{Wu_2006,
  title   = {PnP Problem Revisited},
  volume  = {24},
  issn    = {1573-7683},
  url     = {https://doi.org/10.1007/s10851-005-3617-z},
  doi     = {10.1007/s10851-005-3617-z},
  number  = {1},
  journal = {Journal of Mathematical Imaging and Vision},
  author  = {Wu, Yihong and Hu, Zhanyi},
  year    = {2006},
  month   = jan,
  pages   = {131-141}
}
@misc{opencv_4_5_4,
  author = {Gary Bradski and Adrian Kaehler},
  title = {{OpenCV}},
  howpublished = {\url{https://opencv.org/}},
  year = {2021},
  note = {Version 4.5.4},
}

@inproceedings{Redmon_2016_CVPR,
  author    = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  title     = {You Only Look Once: Unified, Real-Time Object Detection},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2016}
}
@inproceedings{Dong_2018_CVPR,
  author    = {Dong, Xuanyi and Yan, Yan and Ouyang, Wanli and Yang, Yi},
  title     = {Style Aggregated Network for Facial Landmark Detection},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2018}
}
@inproceedings{Sun_2019_CVPR,
  author    = {Sun, Ke and Xiao, Bin and Liu, Dong and Wang, Jingdong},
  title     = {Deep High-Resolution Representation Learning for Human Pose Estimation},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2019}
}
@inproceedings{Newell_2016_ECCV,
  author    = {Newell, Alejandro
               and Yang, Kaiyu
               and Deng, Jia},
  editor    = {Leibe, Bastian
               and Matas, Jiri
               and Sebe, Nicu
               and Welling, Max},
  title     = {Stacked Hourglass Networks for Human Pose Estimation},
  booktitle = {Computer Vision -- ECCV 2016},
  year      = {2016},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {483--499},
  abstract  = {This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a ``stacked hourglass'' network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.},
  isbn      = {978-3-319-46484-8}
}


@inproceedings{Kazemi_2014_CVPR,
  author    = {Kazemi, Vahid and Sullivan, Josephine},
  title     = {One Millisecond Face Alignment with an Ensemble of Regression Trees},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2014}
}
@article{WANG201850,
  title    = {Facial feature point detection: A comprehensive survey},
  journal  = {Neurocomputing},
  volume   = {275},
  pages    = {50-65},
  year     = {2018},
  issn     = {0925-2312},
  doi      = {https://doi.org/10.1016/j.neucom.2017.05.013},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231217308202},
  author   = {Nannan Wang and Xinbo Gao and Dacheng Tao and Heng Yang and Xuelong Li},
  keywords = {Deep learning, Face alignment, Facial feature point detection, Facial landmark localization},
  abstract = {This paper presents a comprehensive survey of facial feature point detection with the assistance of abundant manually labeled images. Facial feature point detection favors many applications such as face recognition, animation, tracking, hallucination, expression analysis and 3D face modeling. Existing methods are categorized into two primary categories according to whether there is the need of a parametric shape model: parametric shape model-based methods and nonparametric shape model-based methods. Parametric shape model-based methods are further divided into two secondary classes according to their appearance models: local part model-based methods (e.g. constrained local model) and holistic model-based methods (e.g. active appearance model). Nonparametric shape model-based methods are divided into several groups according to their model construction process: exemplar-based methods, graphical model-based methods, cascaded regression-based methods, and deep learning based methods. Though significant progress has been made, facial feature point detection is still limited in its success by wild and real-world conditions: large variations across poses, expressions, illuminations, and occlusions. A comparative illustration and analysis of representative methods provides us a holistic understanding and deep insight into facial feature point detection, which also motivates us to further explore more promising future schemes.}
}
